{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "8a1e5624-7ca9-4fcf-9d69-320d4e70bbe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.utils import normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "dab1e488-b6d7-4873-8bc8-90cc31ce1a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train,y_train),(x_test,y_test) = keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "4159f660-98b5-410d-86af-16777415f28d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 28)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = normalize(x_train,axis=1)\n",
    "x_test = normalize(x_test, axis=1)\n",
    "np.shape(x_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "6c50d788-6e18-4d33-b046-cbfac715862c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Flatten(), #(28,28) -> (784,)\n",
    "    Dense(units = 128, activation = 'relu'),\n",
    "    Dense(units = 128, activation = 'relu'),\n",
    "    Dense(units = 10, activation = 'softmax'),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "80691faf-6c0b-43ae-805b-d3e4569a9aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=keras.optimizers.RMSprop(),\n",
    "                loss = SparseCategoricalCrossentropy(),\n",
    "              metrics = ['accuracy']\n",
    "             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "c7d3b347-4069-4e03-a14f-bb812618320e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 847us/step - accuracy: 0.8755 - loss: 0.4398\n",
      "Epoch 2/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 829us/step - accuracy: 0.9633 - loss: 0.1178\n",
      "Epoch 3/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 832us/step - accuracy: 0.9744 - loss: 0.0831\n",
      "Epoch 4/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 836us/step - accuracy: 0.9808 - loss: 0.0603\n",
      "Epoch 5/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 833us/step - accuracy: 0.9844 - loss: 0.0509\n",
      "Epoch 6/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 833us/step - accuracy: 0.9872 - loss: 0.0407\n",
      "Epoch 7/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 836us/step - accuracy: 0.9897 - loss: 0.0370\n",
      "Epoch 8/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 834us/step - accuracy: 0.9914 - loss: 0.0295\n",
      "Epoch 9/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 853us/step - accuracy: 0.9927 - loss: 0.0231\n",
      "Epoch 10/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 796us/step - accuracy: 0.9943 - loss: 0.0203\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x13fd85b70>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train,y_train, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "63efc611-59a5-4d65-9efb-2df93757091d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 844us/step - accuracy: 0.9756 - loss: 0.1374\n",
      "test loss, test acc: [0.11526458710432053, 0.9790999889373779]\n"
     ]
    }
   ],
   "source": [
    "results = model.evaluate(x_test, y_test, batch_size=128)\n",
    "print(\"test loss, test acc:\", results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "e0c89bc3-fc2a-42f0-a6ad-c676689a6389",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling Sequential.call().\n\n\u001b[1mInvalid input shape for input Tensor(\"data:0\", shape=(28, 28), dtype=float32). Expected shape (32, 28, 28), but input has incompatible shape (28, 28)\u001b[0m\n\nArguments received by Sequential.call():\n  • inputs=tf.Tensor(shape=(28, 28), dtype=float32)\n  • training=False\n  • mask=None",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[76], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m random_label \u001b[38;5;241m=\u001b[39m y_test[i]\n\u001b[1;32m      5\u001b[0m np\u001b[38;5;241m.\u001b[39mshape(random_image)\n\u001b[0;32m----> 6\u001b[0m predict \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrandom_image\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/ML/digit_classification/venv/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/Desktop/ML/digit_classification/venv/lib/python3.10/site-packages/keras/src/models/functional.py:273\u001b[0m, in \u001b[0;36mFunctional._adjust_input_rank\u001b[0;34m(self, flat_inputs)\u001b[0m\n\u001b[1;32m    271\u001b[0m             adjusted\u001b[38;5;241m.\u001b[39mappend(ops\u001b[38;5;241m.\u001b[39mexpand_dims(x, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m    272\u001b[0m             \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m--> 273\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    274\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid input shape for input \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Expected shape \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    275\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mref_shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, but input has incompatible shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    276\u001b[0m     )\n\u001b[1;32m    277\u001b[0m \u001b[38;5;66;03m# Add back metadata.\u001b[39;00m\n\u001b[1;32m    278\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(flat_inputs)):\n",
      "\u001b[0;31mValueError\u001b[0m: Exception encountered when calling Sequential.call().\n\n\u001b[1mInvalid input shape for input Tensor(\"data:0\", shape=(28, 28), dtype=float32). Expected shape (32, 28, 28), but input has incompatible shape (28, 28)\u001b[0m\n\nArguments received by Sequential.call():\n  • inputs=tf.Tensor(shape=(28, 28), dtype=float32)\n  • training=False\n  • mask=None"
     ]
    }
   ],
   "source": [
    "i = np.random.randint(len(x_test))\n",
    "random_image = x_test[i]\n",
    "random_label = y_test[i]\n",
    "np.shape(random_image)\n",
    "predict = model.predict(random_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "feac9710-8d7a-4352-bb51-bc1a43699af6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Label:\", random_label)\n",
    "plt.imshow(random_image, cmap=binary)\n",
    "plt.show()\n",
    "print(f\"Predicted Value\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "900354fe-9601-47de-890f-a7b129587f86",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling Sequential.call().\n\n\u001b[1mInvalid input shape for input Tensor(\"data:0\", shape=(32,), dtype=float32). Expected shape (32, 784), but input has incompatible shape (32,)\u001b[0m\n\nArguments received by Sequential.call():\n  • inputs=tf.Tensor(shape=(32,), dtype=float32)\n  • training=False\n  • mask=None",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[40], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m predicted \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrandom_image\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/ML/digit_classification/venv/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/Desktop/ML/digit_classification/venv/lib/python3.10/site-packages/keras/src/models/functional.py:273\u001b[0m, in \u001b[0;36mFunctional._adjust_input_rank\u001b[0;34m(self, flat_inputs)\u001b[0m\n\u001b[1;32m    271\u001b[0m             adjusted\u001b[38;5;241m.\u001b[39mappend(ops\u001b[38;5;241m.\u001b[39mexpand_dims(x, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m    272\u001b[0m             \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m--> 273\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    274\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid input shape for input \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Expected shape \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    275\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mref_shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, but input has incompatible shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    276\u001b[0m     )\n\u001b[1;32m    277\u001b[0m \u001b[38;5;66;03m# Add back metadata.\u001b[39;00m\n\u001b[1;32m    278\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(flat_inputs)):\n",
      "\u001b[0;31mValueError\u001b[0m: Exception encountered when calling Sequential.call().\n\n\u001b[1mInvalid input shape for input Tensor(\"data:0\", shape=(32,), dtype=float32). Expected shape (32, 784), but input has incompatible shape (32,)\u001b[0m\n\nArguments received by Sequential.call():\n  • inputs=tf.Tensor(shape=(32,), dtype=float32)\n  • training=False\n  • mask=None"
     ]
    }
   ],
   "source": [
    "predicted = model.predict(random_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de034ef-f57e-44ef-9ad2-a684caad5c54",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
